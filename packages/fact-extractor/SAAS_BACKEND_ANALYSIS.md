# ESIA Fact Extractor SaaS - Backend Pipeline Analysis & Objectives

**Date**: November 12, 2025
**Status**: Backend Pipeline Ready for Production
**Focus**: Backend pipeline completion (Web frontend, Stripe, database - Phase 2)

---

## Executive Summary

The **ESIA Fact Extractor** is being built as a **Software-as-a-Service (SaaS)** platform for enterprise clients. The backend pipeline is **production-ready** for core functionality (PDF upload â†’ fact extraction â†’ factsheet generation). The system needs additional layers for e-commerce and user management before full production deployment.

---

## My Understanding of Your Objectives

### Primary Vision
Build an **enterprise SaaS platform** that enables environmental consultants and project managers to:
1. Upload **PDF/DOCX documents** (ESIA reports from various projects)
2. **Automatically extract quantitative and categorical facts** using LLM with DSPy
3. **Categorize facts intelligently** into logical project sections (8 categories Ã— 32 subcategories)
4. **Generate project factsheets** organized by category for client deliverables
5. **Track extracted facts** in a database with user editing capabilities
6. **Export results** as CSV or formatted reports

### Business Model
- **SaaS with E-commerce**: Stripe integration for payments
- **Multi-tenant**: Different clients upload different documents
- **Scalable**: Document processing in background
- **White-label ready**: Can be customized for different clients

### Technical Stack
- **Backend**: FastAPI (Python web framework)
- **LLM Integration**: DSPy with multiple providers (Ollama, OpenAI, Claude, Gemini)
- **Document Processing**: Docling (PDF/DOCX â†’ Markdown)
- **Database**: SQLAlchemy ORM (SQLite dev, PostgreSQL production)
- **Task Queue** (Optional): Celery for long-running jobs
- **Frontend** (Future): React/Vue for web interface

### Expected Users
- Environmental consultants
- Project managers
- Corporate sustainability teams
- Government environmental agencies
- Mining/energy sector companies

---

## Current Backend Pipeline Status

### âœ… PRODUCTION-READY COMPONENTS

#### 1. **FastAPI Backend** (saas/backend/main.py)
- **Status**: âœ… Ready for production
- **Features**:
  - File upload endpoint (`/api/upload`)
  - Background processing with progress tracking
  - Job status monitoring (`/api/jobs/{job_id}`)
  - Fact retrieval endpoints
  - CSV export functionality
  - CORS middleware configured
  - Health check endpoint
  - RESTful API design

**Endpoints**:
```
POST   /api/upload                    - Upload PDF/DOCX/MD file
GET    /api/jobs/{job_id}            - Get job status
GET    /api/jobs/{job_id}/facts      - Get extracted facts
GET    /api/jobs/{job_id}/complete   - Get job + facts
PATCH  /api/facts/{fact_id}          - User edits fact
DELETE /api/facts/{fact_id}          - Delete fact
GET    /api/jobs/{job_id}/export/csv - Export to CSV
GET    /health                        - Health check
```

**Database Operations**:
- Stores jobs with status tracking
- Stores extracted facts with metadata
- Supports user edits and comments
- Tracks conflicts and value ranges
- Cascading deletes (job deletion removes facts)

#### 2. **Document Processing** (saas/core/extractor.py)
- **Status**: âœ… Ready for production
- **Features**:
  - Docling integration for PDF/DOCX conversion
  - Multi-provider LLM support
  - Text chunking with progress callback
  - Fact extraction via DSPy
  - Conflict detection (2% tolerance + Ã—10 error detection)
  - Unit normalization (80+ units)
  - Results saved to database

**Processing Pipeline**:
```
1. Configure LLM (Ollama/OpenAI/Claude/Gemini)
2. Load document + Docling conversion
3. Chunk text (4000-char chunks)
4. Extract facts via DSPy (LLM-based)
5. Cluster facts by signature
6. Detect conflicts in values
7. Normalize units to canonical forms
8. Return consolidated facts + metadata
```

#### 3. **Database Schema** (saas/backend/models.py)
- **Status**: âœ… Ready for production
- **Tables**:
  - `jobs`: Processing jobs (status, progress, timestamps)
  - `facts`: Extracted facts with normalized values, conflicts, user edits

**Design**:
- Foreign key relationship (Job â†’ Facts)
- Cascade delete (removing job removes facts)
- Indexed fields for fast queries
- Timestamps for audit trail
- User edit tracking

#### 4. **API Schemas** (saas/backend/schemas.py)
- **Status**: âœ… Ready for production
- **Pydantic models**: Type-safe request/response validation
- **Versioning ready**: Can add API versioning easily

#### 5. **Factsheet Functionality** (esia_extractor.py + improvements)
- **Status**: âœ… Ready for production
- **Features**:
  - LLM-based fact categorization (8 categories Ã— 32 subcategories)
  - Intelligent caching (37-80% hit rate)
  - Confidence scoring (high/medium/low)
  - Error handling and logging
  - Progress tracking with tqdm
  - Cache statistics reporting

**NOT YET INTEGRATED INTO SaaS**:
The factsheet functionality in `esia_extractor.py` is production-ready but NOT integrated into `saas/core/extractor.py`. This is a **gap to fix** before full production.

---

## Critical Gaps for Production SaaS

### 1. âš ï¸ **Factsheet Integration Missing**
**Issue**: Factsheet categorization works in CLI (`esia_extractor.py`) but NOT in SaaS backend
- `saas/core/extractor.py` returns facts WITHOUT categories/subcategories
- Facts don't have: category, subcategory, confidence, rationale
- Database schema doesn't have factsheet fields

**Action Required**:
```python
# Add to Fact model (models.py):
category = Column(String(100))           # Project Overview, etc.
subcategory = Column(String(100))        # Basic Info, etc.
categorization_confidence = Column(String(20))  # high/medium/low
categorization_rationale = Column(Text))

# Update process_document() to:
1. Instantiate FactCategorizer (with caching)
2. Categorize each unique fact
3. Store categorization in database
4. Return categorized facts to frontend
```

**Effort**: Medium (2-4 hours)

---

### 2. âš ï¸ **User Authentication & Multi-tenancy**
**Current State**: No authentication
- All uploads are anonymous
- No user accounts
- No API key authentication
- No tenant isolation

**Action Required**:
```python
# Add to models.py:
class User(Base):
    id = Column(Integer, primary_key=True)
    email = Column(String, unique=True)
    password_hash = Column(String)
    subscription_tier = Column(String)  # free, pro, enterprise
    created_at = Column(DateTime)

class Job(Base):
    user_id = Column(Integer, ForeignKey("users.id"))  # Add this
    # ... rest of fields

# Add authentication endpoints:
POST   /auth/register           - Sign up
POST   /auth/login              - Login (returns JWT)
POST   /auth/refresh            - Refresh token
GET    /auth/me                 - Current user info

# Add middleware:
- JWT token validation
- User isolation (can only see own jobs)
```

**Effort**: Medium (3-5 hours)

---

### 3. âš ï¸ **Payment Integration (Stripe)**
**Current State**: Not implemented

**Action Required**:
```python
# Add to models.py:
class Subscription(Base):
    user_id = Column(Integer, ForeignKey("users.id"))
    stripe_customer_id = Column(String)
    stripe_subscription_id = Column(String)
    tier = Column(String)  # free, pro, enterprise
    status = Column(String)  # active, canceled, past_due
    current_period_end = Column(DateTime)

# Add Stripe endpoints:
POST   /stripe/webhook                    - Webhook for events
POST   /billing/create-subscription       - Start subscription
POST   /billing/cancel-subscription       - Cancel subscription
GET    /billing/customer-portal          - Stripe customer portal

# Add usage tracking:
- Track documents processed per month
- Enforce limits by tier (free: 5/mo, pro: 100/mo, enterprise: unlimited)
```

**Effort**: High (5-7 hours)

---

### 4. âš ï¸ **Web Frontend**
**Current State**: Placeholder in main.py (tries to mount /static)

**Action Required**:
```
Create React/Vue SPA:
â”œâ”€â”€ Login/Register pages
â”œâ”€â”€ Dashboard (upload, job list)
â”œâ”€â”€ Job detail (view facts, edit, export)
â”œâ”€â”€ Settings (subscription, API keys)
â””â”€â”€ Admin panel (if needed)

Integration with backend:
- Upload file â†’ show progress
- Fetch job status â†’ real-time updates
- Edit facts â†’ send PATCH requests
- Export CSV â†’ trigger download
- Manage subscription â†’ Stripe link
```

**Effort**: Very High (15-20 hours for basic MVP)

---

### 5. âš ï¸ **Production Deployment Setup**
**Current State**: Development only

**Action Required**:
```
- Docker containerization
- Environment configuration (prod .env)
- PostgreSQL instead of SQLite
- Celery + Redis for background jobs
- S3/cloud storage for uploads
- SSL/TLS certificates
- Rate limiting
- Logging and monitoring
```

**Effort**: High (8-10 hours)

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WEB FRONTEND (Future)                      â”‚
â”‚                 (React/Vue SPA with Stripe UI)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/HTTPS
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI BACKEND                              â”‚
â”‚                 (saas/backend/main.py)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Routes:                                                           â”‚
â”‚  â€¢ /api/upload (POST) - File upload                              â”‚
â”‚  â€¢ /api/jobs/* (GET) - Job status                                â”‚
â”‚  â€¢ /api/facts/* (PATCH/DELETE) - User edits                      â”‚
â”‚  â€¢ /auth/* (POST) - Login/Register                               â”‚
â”‚  â€¢ /stripe/* (POST) - Payment webhooks                           â”‚
â”‚  â€¢ /health (GET) - Health check                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database    â”‚ â”‚ Task Queue   â”‚ â”‚ Stripe API   â”‚
â”‚ (SQLAlchemy) â”‚ â”‚  (Celery)    â”‚ â”‚              â”‚
â”‚ (SQLite/PG)  â”‚ â”‚              â”‚ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Background Processing Worker â”‚
         â”‚  (saas/core/extractor.py)     â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ 1. Docling (PDFâ†’Markdown)     â”‚
         â”‚ 2. Chunking                   â”‚
         â”‚ 3. Fact Extraction (DSPy)     â”‚
         â”‚ 4. Categorization (DSPy) âš ï¸   â”‚
         â”‚ 5. Unit Normalization         â”‚
         â”‚ 6. Conflict Detection         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚     â”‚     â”‚
                 â†“     â†“     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   LLM Providers          â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚ â€¢ Ollama (local)         â”‚
         â”‚ â€¢ OpenAI                 â”‚
         â”‚ â€¢ Claude (Anthropic)     â”‚
         â”‚ â€¢ Gemini (Google)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow: Document Upload to Factsheet

```
1. USER UPLOADS FILE
   â†“
   Client â†’ POST /api/upload (PDF/DOCX/MD)
   â†“
   Server stores file, creates Job record (status: pending)
   â†“
   Returns: job_id, session_id

2. BACKGROUND PROCESSING STARTS
   â†“
   Worker: process_document(file_path)
   â”œâ”€ Configure LLM provider
   â”œâ”€ Docling converts PDF/DOCX â†’ Markdown
   â”œâ”€ Chunk markdown (4000-char chunks)
   â”œâ”€ FactExtractor: Extract facts (DSPy)
   â”œâ”€ Cluster by signature
   â”œâ”€ Detect conflicts
   â”œâ”€ Normalize units
   â””â”€ FactCategorizer: Categorize facts (DSPy) âš ï¸ MISSING

3. STORE IN DATABASE
   â†“
   For each fact:
   â”œâ”€ signature, name, type
   â”œâ”€ value_raw, value_num, unit_raw
   â”œâ”€ value_normalized, unit_normalized
   â”œâ”€ evidence, page, chunk_id
   â”œâ”€ occurrence_count, has_conflict, conflict_description
   â”œâ”€ category, subcategory, confidence, rationale âš ï¸ MISSING
   â””â”€ Store in Fact table

4. USER VIEWS RESULTS
   â†“
   Client â†’ GET /api/jobs/{job_id}/complete
   â†“
   Return: Job + all facts with metadata
   â†“
   Frontend: Display facts organized by category

5. USER EDITS & EXPORTS
   â†“
   Client â†’ PATCH /api/facts/{fact_id} (correct values)
   Client â†’ GET /api/jobs/{job_id}/export/csv
   â†“
   Return: CSV file with facts
```

---

## File Structure

```
/home/user/esia-fact-extractor/
â”œâ”€â”€ esia_extractor.py                   # CLI tool (with factsheet)
â”œâ”€â”€ FACTSHEET_TEST_REPORT.md            # Test results
â”œâ”€â”€ IMPLEMENTATION_CHANGES_SUMMARY.md   # Local changes
â”œâ”€â”€ PIPELINE_OVERVIEW.md                # Architecture docs
â”œâ”€â”€ QUICK_START_LOCAL.md                # User guide
â”œâ”€â”€ README.md                           # Project docs
â”œâ”€â”€ CLAUDE.md                           # Dev guidance
â”‚
â”œâ”€â”€ saas/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ extractor.py               # Document processing âœ… (missing factsheet integration)
â”‚   â”‚
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ main.py                    # FastAPI app âœ…
â”‚       â”œâ”€â”€ database.py                # SQLAlchemy setup âœ…
â”‚       â”œâ”€â”€ models.py                  # DB models (missing factsheet fields) âš ï¸
â”‚       â”œâ”€â”€ schemas.py                 # Pydantic schemas (missing factsheet) âš ï¸
â”‚       â”œâ”€â”€ celery_app.py             # Celery config (optional)
â”‚       â””â”€â”€ test_*.py                  # Test files
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## What's Ready vs. What's Missing

### âœ… PRODUCTION-READY (Ready to Deploy)
- FastAPI backend structure
- Database schema (Job, Fact tables)
- File upload endpoint
- Background job processing
- Docling integration (PDF/DOCX â†’ Markdown)
- Fact extraction (DSPy-based)
- Conflict detection
- Unit normalization
- CSV export
- Health check endpoint
- API schemas (Pydantic)

### âš ï¸ NEEDS COMPLETION (Before Production)
1. **Factsheet Integration** (Easy - 2-4 hours)
   - Add category/subcategory fields to Fact model
   - Integrate FactCategorizer into process_document()
   - Add endpoints to return factsheet data

2. **Authentication** (Medium - 3-5 hours)
   - User registration/login
   - JWT token generation/validation
   - User isolation (job permissions)

3. **Stripe Integration** (High - 5-7 hours)
   - Subscription management
   - Webhook handlers
   - Usage tracking and rate limiting
   - Billing portal link

4. **Web Frontend** (Very High - 15-20 hours)
   - React/Vue SPA
   - Upload UI
   - Job tracking
   - Fact viewing/editing
   - Export functionality
   - Subscription management

5. **Production Deployment** (High - 8-10 hours)
   - Docker setup
   - PostgreSQL configuration
   - Environment variables
   - Celery + Redis (if async jobs needed)
   - Cloud storage (S3)
   - SSL/TLS
   - Monitoring/logging

### ğŸš€ FUTURE ENHANCEMENTS
- Mobile app (React Native/Flutter)
- Advanced analytics (fact trends)
- Custom categorization per client
- API for third-party integrations
- Webhook notifications
- Bulk processing
- Template library for ESIA formats

---

## Production Readiness Checklist

### Phase 1: Backend Pipeline (Current)
- âœ… FastAPI server running
- âœ… Database models defined
- âœ… File upload working
- âœ… Document processing pipeline
- âœ… Fact extraction with DSPy
- âš ï¸ **TODO**: Add factsheet categorization to SaaS
- âš ï¸ **TODO**: Add user authentication
- âš ï¸ **TODO**: Add Stripe integration

### Phase 2: Frontend & E-Commerce (Next)
- â³ Web frontend (React/Vue)
- â³ Stripe subscription handling
- â³ User dashboard
- â³ Admin panel

### Phase 3: Production Deployment (Final)
- â³ Docker containerization
- â³ PostgreSQL database
- â³ Celery + Redis
- â³ Cloud storage
- â³ SSL/TLS
- â³ Monitoring

---

## Estimated Timeline

| Component | Effort | Timeline |
|-----------|--------|----------|
| Factsheet Integration | 2-4 hrs | 1 day |
| Authentication | 3-5 hrs | 1-2 days |
| Stripe Integration | 5-7 hrs | 2-3 days |
| Frontend MVP | 15-20 hrs | 1 week |
| Production Setup | 8-10 hrs | 2-3 days |
| **TOTAL** | **33-46 hrs** | **2-3 weeks** |

---

## Recommendations for Next Steps

### Immediate (This Sprint)
1. **Integrate factsheet categorization into SaaS** (highest priority)
   - Update `models.py`: Add category, subcategory, confidence, rationale fields
   - Update `saas/core/extractor.py`: Add FactCategorizer integration
   - Update `schemas.py`: Include factsheet fields in API responses
   - Test end-to-end pipeline

2. **Add user authentication** (critical for multi-tenancy)
   - User registration/login endpoints
   - JWT token handling
   - Database user table
   - Job permission checks

### Short-term (Next Sprint)
3. **Implement Stripe integration**
   - Subscription model in database
   - Webhook handler for events
   - Usage tracking per user/tier
   - Free tier: 5 docs/month, Pro: 100/month, Enterprise: unlimited

4. **Start frontend development**
   - Basic React app with upload
   - Job status tracking
   - Results viewing

### Medium-term (Month 2)
5. **Production deployment setup**
   - Docker, PostgreSQL, S3
   - Environment configuration
   - Monitoring and logging

6. **Complete frontend**
   - User dashboard
   - Settings/billing
   - Export functionality
   - Mobile responsiveness

---

## Conclusion

Your **ESIA Fact Extractor SaaS backend pipeline is solid and production-ready** for core functionality. The main gaps are:

1. **Factsheet integration** (quick win - add categorization to SaaS)
2. **Authentication** (required for multi-tenancy)
3. **Stripe payments** (required for SaaS business model)
4. **Web frontend** (required for users to interact)
5. **Production infrastructure** (required for reliability/scale)

The estimated timeline to **full production-ready SaaS** is **2-3 weeks** with focused effort.

**Recommendation**: Start with factsheet integration and authentication this week, then add Stripe and frontend next week for a complete MVP.
