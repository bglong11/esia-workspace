# Token-Based Cost Estimation Guide

## Overview

The ESIA pipeline uses Large Language Models (LLMs) to extract structured facts from environmental and social impact assessment documents. LLM APIs charge based on the number of **tokens** processed - both input tokens (text sent to the API) and output tokens (text generated by the API).

This guide explains how to estimate API costs for your ESIA pipeline runs using the token data that's automatically collected during processing.

**Current Default Model:** Gemini 2.5 Flash (Google AI)

---

## Why Cost Estimation Matters

- **Budget Planning:** Understand costs before processing large document batches
- **Model Selection:** Compare costs between different LLM providers
- **Optimization:** Identify opportunities to reduce token usage
- **Transparency:** Track spending on AI-powered document analysis

---

## How Token Counting Works in the Pipeline

### Step 1: Document Chunking

**File:** `esia-fact-extractor-pipeline/step1_docling_hybrid_chunking.py`

During document chunking, the pipeline:

1. **Tokenizes** each chunk using OpenAI's `tiktoken` library (with `gpt-4o` tokenizer model)
2. **Counts** exact tokens for each chunk
3. **Stores** token count in the output files

**Where token data is saved:**

- **Per-chunk tokens:** `data/outputs/{ROOT_NAME}_chunks.jsonl`
  - Each line contains a chunk with its `token_count` field

- **Aggregate statistics:** `data/outputs/{ROOT_NAME}_meta.json`
  - Contains `statistics.total_tokens`, `min_tokens`, `max_tokens`

### Step 2: Fact Extraction

**File:** `esia-fact-extractor-pipeline/step3_extraction_with_archetypes.py`

During fact extraction:

1. Chunks are **grouped by section** (e.g., "Project Description", "Environmental Baseline")
2. All chunks in a section are **combined** into one text string
3. One **LLM API call** is made per section to extract facts
4. The LLM generates structured output (up to 2000 tokens per extraction)

---

## Finding Your Token Data

After running Step 1 (chunking), locate the metadata file:

```
packages/pipeline/data/outputs/{ROOT_NAME}_meta.json
```

**Example:** For a PDF named `WHW_ESIA.pdf`, the metadata file would be:
```
packages/pipeline/data/outputs/1764623841532-WHW_ESIA_meta.json
```

**Open the file** and look for the `statistics` section:

```json
{
  "filename": "WHW_ESIA.pdf",
  "statistics": {
    "total_tokens": 20664,
    "total_chunks": 782,
    "min_tokens": 15,
    "max_tokens": 2500,
    "avg_chunk_length": 1234
  }
}
```

**Key fields:**
- `total_tokens`: Sum of all chunk tokens (this is your **input token count** for Step 2)
- `total_chunks`: Number of chunks created
- `min_tokens` / `max_tokens`: Token range per chunk

---

## Gemini 2.5 Flash Pricing

**Official Pricing (as of January 2025):**

| Token Type | Price per 1M Tokens | Price per Token |
|------------|---------------------|-----------------|
| **Input** | $0.075 | $0.000000075 |
| **Output** | $0.30 | $0.0000003 |
| **Cached Input** | $0.01875 (75% discount) | $0.00000001875 |

**Source:** [Google AI Pricing](https://ai.google.dev/pricing)

**Notes:**
- Pricing is subject to change - always verify current rates
- Free tier available with rate limits
- Context caching can reduce costs for repeated queries

---

## Step-by-Step Cost Calculation

### Step 1: Get Input Token Count

From your metadata file (`{ROOT_NAME}_meta.json`):

```json
"total_tokens": 20664
```

### Step 2: Count Unique Sections

Open your chunks file (`{ROOT_NAME}_chunks.jsonl`) and count unique section names.

**Quick method using Python:**

```python
import json

sections = set()
with open('data/outputs/{ROOT_NAME}_chunks.jsonl', 'r', encoding='utf-8') as f:
    for line in f:
        chunk = json.loads(line)
        sections.add(chunk.get('section', 'Unknown'))

num_sections = len(sections)
print(f"Unique sections: {num_sections}")
```

**Quick method using PowerShell:**

```powershell
Get-Content data/outputs/{ROOT_NAME}_chunks.jsonl |
    ForEach-Object { (ConvertFrom-Json $_).section } |
    Sort-Object -Unique |
    Measure-Object |
    Select-Object -ExpandProperty Count
```

### Step 3: Calculate Input Cost

```
Input Cost = (total_tokens × $0.075) / 1,000,000
```

**Example:**
```
Input Cost = (20,664 × 0.075) / 1,000,000
Input Cost = $0.001549 ≈ $0.0015
```

### Step 4: Estimate Output Tokens

The pipeline is configured to generate up to **2000 tokens** per extraction.

**Formula:**
```
Estimated Output Tokens = num_sections × 2000
```

**Example:**
```
Estimated Output Tokens = 640 × 2000 = 1,280,000 tokens
```

**Note:** This is a **conservative estimate**. Actual output may be less if extractions are shorter.

### Step 5: Calculate Output Cost

```
Output Cost = (output_tokens × $0.30) / 1,000,000
```

**Example:**
```
Output Cost = (1,280,000 × 0.30) / 1,000,000
Output Cost = $0.384
```

### Step 6: Calculate Total Cost

```
Total Cost = Input Cost + Output Cost
```

**Example:**
```
Total Cost = $0.0015 + $0.384 = $0.3855 ≈ $0.39
```

---

## Worked Example: 869-Page ESIA Document

**Document:** WHW ESIA.pdf (869 pages)

**Step 1 Output:**
- Chunks created: 782
- Total tokens: 20,664
- Unique sections: 640

**Cost Breakdown:**

| Component | Calculation | Cost |
|-----------|-------------|------|
| **Input Tokens** | 20,664 tokens × $0.075 / 1M | $0.0015 |
| **Output Tokens** | 640 sections × 2000 tokens × $0.30 / 1M | $0.384 |
| **Total** | Input + Output | **$0.39** |

**Conclusion:** Processing this 869-page document costs approximately **$0.39 USD** with Gemini 2.5 Flash.

---

## Cost Optimization Tips

### 1. Reduce Chunk Size
**Current default:** 2500 tokens per chunk

**Why it helps:** Smaller chunks reduce redundancy when sections span multiple chunks.

**How to change:**
```bash
python step1_docling_hybrid_chunking.py document.pdf --chunk-max-tokens 1500
```

### 2. Filter Irrelevant Sections
Before extraction, manually review chunks and exclude sections like:
- Table of contents
- Appendices
- Glossaries
- Acknowledgments

### 3. Use Free Tier Models for Testing

**Gemini 2.0 Flash Exp:**
- **Cost:** Free (with rate limits)
- **How to use:**
  ```bash
  export GOOGLE_MODEL="gemini-2.0-flash-exp"
  ```

**Trade-off:** Experimental models may have lower quality or availability.

### 4. Leverage Context Caching

For repeated queries on the same document:
- First query: Full input cost
- Subsequent queries: 75% discount on cached portions

**Cost:** $0.01875 per 1M cached tokens (vs $0.075 for fresh input)

### 5. Batch Similar Documents

Process multiple documents in one session to:
- Amortize initialization costs
- Reuse loaded models
- Optimize API rate limits

---

## Alternative Models & Pricing

### Gemini Models (Google AI)

| Model | Input (per 1M) | Output (per 1M) | Notes |
|-------|---------------|----------------|-------|
| **gemini-2.5-flash** | $0.075 | $0.30 | Current default, fast |
| **gemini-2.0-flash-exp** | Free | Free | Experimental, rate limited |
| **gemini-1.5-flash** | $0.075 | $0.30 | Older version |
| **gemini-1.5-pro** | $1.25 | $5.00 | Higher quality, 10x cost |

### Claude Models (via OpenRouter)

| Model | Input (per 1M) | Output (per 1M) | Notes |
|-------|---------------|----------------|-------|
| **claude-3-haiku** | $0.25 | $1.25 | Fast, affordable |
| **claude-3-sonnet** | $3.00 | $15.00 | Balanced |
| **claude-3-opus** | $15.00 | $75.00 | Highest quality |

**How to switch to Claude:**

```bash
# Set environment variables
export LLM_PROVIDER="openrouter"
export OPENROUTER_API_KEY="your_key_here"
export OPENROUTER_MODEL="anthropic/claude-3-haiku"

# Run extraction
python step3_extraction_with_archetypes.py --chunks data/outputs/{ROOT_NAME}_chunks.jsonl
```

### Cost Comparison Example

For the 869-page WHW ESIA document:

| Model | Input Cost | Output Cost | Total Cost |
|-------|-----------|-------------|------------|
| **Gemini 2.5 Flash** | $0.0015 | $0.384 | **$0.39** |
| **Gemini 2.0 Flash Exp** | Free | Free | **$0.00** |
| **Claude 3 Haiku** | $0.0052 | $1.60 | **$1.61** |
| **Claude 3 Sonnet** | $0.062 | $19.20 | **$19.26** |

---

## Python Script for Automated Cost Calculation

Save this script as `estimate_costs.py`:

```python
#!/usr/bin/env python3
"""
Quick cost estimator for ESIA pipeline
Usage: python estimate_costs.py data/outputs/{ROOT_NAME}
"""

import json
import sys
from pathlib import Path

# Pricing (update as needed)
PRICING = {
    'gemini-2.5-flash': {
        'input': 0.075,   # per 1M tokens
        'output': 0.30,   # per 1M tokens
    },
    'gemini-2.0-flash-exp': {
        'input': 0.0,
        'output': 0.0,
    },
    'claude-3-haiku': {
        'input': 0.25,
        'output': 1.25,
    },
}

def estimate_cost(root_name, model='gemini-2.5-flash'):
    # Read metadata
    meta_file = Path(f'{root_name}_meta.json')
    if not meta_file.exists():
        print(f"Error: Metadata file not found: {meta_file}")
        return

    with open(meta_file, 'r', encoding='utf-8') as f:
        meta = json.load(f)

    total_tokens = meta['statistics']['total_tokens']

    # Count unique sections
    chunks_file = Path(f'{root_name}_chunks.jsonl')
    if not chunks_file.exists():
        print(f"Error: Chunks file not found: {chunks_file}")
        return

    sections = set()
    with open(chunks_file, 'r', encoding='utf-8') as f:
        for line in f:
            chunk = json.loads(line)
            sections.add(chunk.get('section', 'Unknown'))

    num_sections = len(sections)
    estimated_output_tokens = num_sections * 2000  # Max tokens per extraction

    # Calculate costs
    pricing = PRICING[model]
    input_cost = (total_tokens * pricing['input']) / 1_000_000
    output_cost = (estimated_output_tokens * pricing['output']) / 1_000_000
    total_cost = input_cost + output_cost

    # Print report
    print("=" * 60)
    print("ESIA PIPELINE COST ESTIMATION")
    print("=" * 60)
    print(f"Model: {model}")
    print(f"Document: {meta['filename']}")
    print()
    print("INPUT TOKENS:")
    print(f"  Total chunks: {meta['statistics']['total_chunks']}")
    print(f"  Total tokens: {total_tokens:,}")
    print(f"  Input cost: ${input_cost:.4f}")
    print()
    print("OUTPUT TOKENS (Estimated):")
    print(f"  Unique sections: {num_sections}")
    print(f"  Est. output tokens: {num_sections} × 2000 = {estimated_output_tokens:,}")
    print(f"  Output cost: ${output_cost:.4f}")
    print()
    print("=" * 60)
    print(f"TOTAL ESTIMATED COST: ${total_cost:.4f}")
    print("=" * 60)
    print()
    print("Note: This is an estimate. Actual costs may vary based on:")
    print("  - Shorter actual outputs (estimate assumes max 2000 tokens)")
    print("  - API rate limits and retries")
    print("  - Pricing changes by provider")

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python estimate_costs.py data/outputs/{ROOT_NAME}")
        print("Example: python estimate_costs.py data/outputs/1764623841532-WHW_ESIA")
        sys.exit(1)

    root_name = sys.argv[1]
    model = sys.argv[2] if len(sys.argv) > 2 else 'gemini-2.5-flash'

    estimate_cost(root_name, model)
```

**Usage:**

```bash
# Basic usage with default model (Gemini 2.5 Flash)
python estimate_costs.py data/outputs/1764623841532-WHW_ESIA

# Specify a different model
python estimate_costs.py data/outputs/1764623841532-WHW_ESIA claude-3-haiku
```

---

## References

### Pipeline Configuration Files

- **Token Counting:** `esia-fact-extractor-pipeline/step1_docling_hybrid_chunking.py` (lines 525-528)
- **Chunk Processing:** `esia-fact-extractor-pipeline/step3_extraction_with_archetypes.py` (lines 85-172)
- **LLM Configuration:** `esia-fact-extractor-pipeline/src/config.py` (lines 48-52)
- **Model Settings:** `esia-fact-extractor-pipeline/src/esia_extractor.py` (lines 155)

### Environment Variables

- `GOOGLE_MODEL` - Set Gemini model (default: `gemini-2.5-flash`)
- `GOOGLE_API_KEY` - Google AI API key (required)
- `LLM_PROVIDER` - Choose provider: `google` or `openrouter`
- `OPENROUTER_API_KEY` - OpenRouter API key (for Claude models)
- `LLM_MAX_TOKENS` - Max output tokens (default: 2000)
- `LLM_TEMPERATURE` - Generation temperature (default: 0.3)

### External Documentation

- **Google AI Pricing:** https://ai.google.dev/pricing
- **Google AI Models:** https://ai.google.dev/models
- **OpenRouter Pricing:** https://openrouter.ai/docs#models
- **Tiktoken (OpenAI):** https://github.com/openai/tiktoken

---

## Frequently Asked Questions

### Q: Why does token count differ from word count?

**A:** Tokens are subword units. Common words = 1 token, uncommon words may be 2-3 tokens. Numbers, symbols, and technical terms often split into multiple tokens.

**Example:**
- "environmental" → 2 tokens (`environ`, `mental`)
- "assessment" → 1 token
- "ESIA" → 1 token (in some tokenizers)

### Q: Can I reduce costs by using a different tokenizer?

**A:** The tokenizer used for **counting** (tiktoken with gpt-4o) doesn't affect your actual API costs. It's only used to estimate chunk sizes. Your actual costs depend on the tokenizer used by the LLM model (Gemini, Claude, etc.).

### Q: Why is output cost estimated, not exact?

**A:** We don't know how many tokens the LLM will generate until it runs. We use the configured `max_tokens` (2000) as a conservative upper bound. Actual output is often shorter, making real costs lower than estimates.

### Q: How accurate are these cost estimates?

**A:**
- **Input cost:** Very accurate (based on exact token counts)
- **Output cost:** Conservative estimate (assumes maximum output)
- **Typical variance:** ±10-20% depending on actual output length

### Q: Does the pipeline track actual API costs?

**A:** Not currently. The pipeline tracks token counts but not real-time API costs. You can implement this by logging API responses, which often include usage metadata.

---

## Summary

**To estimate ESIA pipeline costs:**

1. ✅ Run Step 1 (chunking) on your document
2. ✅ Open `{ROOT_NAME}_meta.json` and note `total_tokens`
3. ✅ Count unique sections in `{ROOT_NAME}_chunks.jsonl`
4. ✅ Calculate: `Input Cost = (total_tokens × 0.075) / 1M`
5. ✅ Calculate: `Output Cost = (sections × 2000 × 0.30) / 1M`
6. ✅ Total Cost = Input + Output

**Or use the provided Python script** for automated calculation.

---

**Last Updated:** December 2025
**Pipeline Version:** 1.0
**Default Model:** Gemini 2.5 Flash
